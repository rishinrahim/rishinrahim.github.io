<!DOCTYPE html>
<!-- saved from url=(0030)https://karpathy.ai/books.html -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Rishin Rahim: Books</title>
    <link rel="stylesheet" type="text/css" href="../style.css">
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
    <style>
      .books {
        margin-top: 10px;
      }
      .book {
        margin-bottom: 10px;
      }
      .book-title {
        font-weight: bold;
      }
      .book-detail {
        color: #666;
      }
      .book-text {
        margin-left: 5px;
      }
    </style>
  </head>
  <body>

    <div class="container">
      <h1>30 Days of Statistics</h1>
      <hr>

      <h3>Day 1: Basics of Measurement</h3>
      <p>June 1, 2021</p>
      <ul>
      <li>Measurement is the process of assigning numbers to objects to facilitate the use of mathematics.</li>
      <li><b>Levels of Measurement</b>: Nominal and Ordinal data, Interval and Ratio Data, Continuous and discrete Data</li>
      <li><b>Operationalization :</b>The process of specifying how a concept will be defined and measured.
        For example, intelligence has no definite metric; instead, we use IQ score to measure intelligence.
        Other examples include disaster preparedness, quality of life, pain, etc.
      </li>
      <li><b>Proxy Measurement :</b>The process of substituting one measurement with another.</li>
      <li><b>Errors :</b>Reducible errors / systemic errors and Irreducible errors / random errors</li>
      <li><b>Evaluation of Methods of Measurement :</b>Reducible errors / systemic errors and Irreducible errors / random errors</li>
      </ul>

      <hr>
      <h3>Day 2: Evaluation of Methods of measurement</h3>
      <p>June 2, 2021</p>
      <ul>
      <li>Measurement is the process of assigning numbers to objects to facilitate the use of mathematics.</li>
      <li><b>Reliability</b>:How consistent or repeated measurements are.
        Many measures of reliability draw on the correlation coefficient.
        Three primary approaches: multiple occasion reliability, multiple forms reliability, and internal consistency.
      </li>
      <li><b>Validity :</b>Multiple form: How similarly different versions of testing perform in measuring the same entity.
        Multiple form: How similarly different versions of testing perform in measuring the same entity.
        Internal consistency: How well the items that make up an instrument reflect the same construct.
      </li>
      <li><b>Triangulation :</b>The process of combining different information
         from different sources to arrive at a true or, at least, most accurate value. For example, 
         Multi-method matrix (MTMM)</li>
      </ul>
    
      <hr>
      <h3>Day 3: Method of Central Tendency</h3>
      <p>June 3, 2021</p>
      <ul>
        <li>Among the first statistics computed for the continuous variables in a new dataset. These measures indicate where most values in a distribution fall .</li>
        <li>The three most common measures of central tendency are the mean, the median, and the mode. One’s choice of mean, median, or mode can dramatically change the interpretation of the data.</li>
        <li>The mean is the arithmetic average (the sum of the scores divided by the number of scores). While the mean is the most common measure of a dataset’s center, there are situations when it cannot be used.</li>
        <li>Mean is not an appropriate summary measure for every dataset because it is sensitive to extreme values (outliers), so can be misleading for Skewed data.</li>
        <li>Trimmed Mean also known as Winsorized Mean is calculated by discarding certain percentage of extreme values in a distribution and calculating mean of the remaining values.</li>
        <li>When the data are nominal (i.e., when the data are categories rather than values), you must use the mode (Most frequently occurring value) to summarize the center</li>
        <li>Median is the best option when data are ordinal. The median of a distribution of scores is the value at the 50th percentile, which means that half of the scores are below this value and half are above it.</li>
        <li>To summarize interval or ratio data, in general, you should use the mean. however, if the data set contains one or more outliers, you should use the median.</li>
        <li>If mean and median are close to each other, and the most common ranges cluster around the mean , then we can conclude that data is Normal and Symmetrical</li>
        <li>A mean lower than the median is typical of left-skewed data because the extreme lower values pull the mean down, whereas they do not have the same effect on the median</li>
        <li>A mean higher than a median is common for right-skewed data because the extreme higher values pull the mean up but do not have the same effect on the median.</li>

      </ul>
      <hr>

      <h3>Day 4: Percentiles</h3>
      <p>June 4, 2021</p>
      <ul>
        <li>In real world environments, performance gets attention when it is poor and has a negative impact on the business and users. But how can we identify performance issues quickly to prevent negative effects?</li>
        <li>We cannot alert on every slow transaction, since there are always some. The industry has come up with a solution called Automatic Baselining. Baselining calculates out the “normal” performance and only alerts us when an application slows down or produces more errors than us</li>
        <li>Most approaches rely on averages and standard deviations. This approach assumes that the response times are distributed over a bell curve. Averages , in this case, are ineffective because they are too simplistic and one-dimensional.</li>
        <li>A percentile gives a much better sense of real world performance, it tells me at which part of the curve I am looking at and how many transactions are represented by that metric.</li>
        <li>For exactly that reason percentiles are perfect for automatic baselining. If the 50th percentile moves from 500ms to 600ms I know that 50% of my transactions suffered a 20% performance degradation. You need to react to that.</li>
        <li>Percentile-based alerts do not suffer from false positives, are a lot less volatile and don’t miss any important performance degradations! Consequently, a baselining approach that uses percentiles does not require a lot of tuning variables to work effectively.</li>
      </ul>

      Reference: <a href="https://www.dynatrace.com/news/blog/why-averages-suck-and-percentiles-are-great/#:~:text=Averages%20are%20ineffective%20because%20they,performance%20characteristics%20of%20your%20application.">Why Averages Suck and Percentiles are Great</a> 

      <hr>

      <h3>Day 5: Discrete and Continuous Random Variables      </h3>
      <p>June 5, 2021</p>
      <ul>
      <li>A <b>Variable</b> is a quantity whose value changes</li>  
      <li>A <b>Discrete variable</b> is a variable whose value is obtained by counting. eg: Number of students in a class, number of marbles in a jar</li>  
      <li>A <b>Continuous Variable</b> is a variable whose value is obtained by measuring eg: Height of students in a class</li>  
      <li>A <b>random variable</b> is a variable whose value is determined by random chance. A random variable is denoted by a capital letter
        Probability distribution of a random variable X tells what the possible values of X are and how probabilities are assigned to those values.
        Random variable can be discrete or continuous
      </li>  
      <li>A <b>Discrete random variable X</b> has countable number of possible values. To graph the probability distribution of X, we construct probability histogram      </li>  
      <li>A <b>Continuous random variable X</b> takes all values in a given interval of values. 
        The Probability distribution of a continuous random variable is shown by a density curve.
        The Probability that X is between an interval of numbers is the area under the density curve between the interval points.
        The probability that X is exactly equal to a number is Zero.

    </li>  
      <li><b>Expected Value(\(E_x\))</b>: If you have a collection of numbers, \(a_1 \ldots a_n\), then the average of the numbers is a representation of the collection. Now consider a collection of random variables \(X\), then the average of the random variables is called the <b>Expected Value</b>. 
       
  <p>To understand the concept behind \(E_X\), consider a discrete random variable with range \(R_X = \{x_1, x_2, x_3, \ldots\}\).
     This random variable is a result of a random experiment. Suppose that we repeat this experiment a very large number of times \(N\), 
     and that the trials are independent. Let \(N_1\) be the number of times we observe \(x_1\), \(N_2\) be the number of times we observe
      \(x_2\), \(N_k\) be the number of times we observe \(x_k\), and so on. Since \(P(X=x_k) = P_X(x_k)\), we expect that</p>

  <p>$$P_X(x_1) \approx \frac{N_1}{N}$$</p>
  <p>$$P_X(x_2) \approx \frac{N_2}{N}$$</p>
  <p>$$P_X(x_k) \approx \frac{N_k}{N}$$</p>

  <p>In other words, we have \(N_k \approx N \cdot P_X(x_k)\). Now, if we take the average of the observed values of \(X\), we obtain</p>

  <p>$$\text{Average} = \frac{N_1x_1 + N_2x_2 + N_3x_3 + \ldots}{N}$$</p>
  <p>$$= \frac{N \cdot P_X(x_1) \cdot x_1 + N \cdot P_X(x_2) \cdot x_2 + \ldots}{N}$$</p>
  <p>$$= P_X(x_1) \cdot x_1 + P_X(x_2) \cdot x_2 + \ldots$$</p>
  <p>$$= \sum P_X(x_i) \cdot x_i$$</p>
    <p>Thus, the intuition behind \(E_X\) is that if you repeat the random experiment independently \(N\) times and take the average of the observed data, the average gets closer and closer to \(E_X\) as \(N\) gets larger and larger.</p>
  </li>         

      </ul>


      <hr>

      <h3>Day 6: Likelihood </h3>
      <p>June 6, 2021</p>
      <ul>
        <li>Likelihood is different for discrete and continuous random variable</li>
        <li><b>Discrete Random Variables : </b>Suppose that you have a stochastic process that takes discrete values ( eg. Outcomes of tossing of coin 10 times)
          In such cases, we calculate probability of particular set of outcomes by making assumptions about the underlying stochastic process
           ( eg Probability of landing Heads is \(p\) and that coin tosses are independent). 
           Let the observed outcomes is \(O\) and set of parameters that describe the stochastic process as \(\theta\), then when we speak about probability we mean \(P(O|\theta)\)
           <p>However, when we model real-life stochastic processes, we often do not know \(\theta\); we simply observe \(O\), and then our goal is to estimate \(\theta\).</p>
           <p>We know that given a value of \(\theta\), the probability of observing \(O\) is \(P(O|\theta)\). Thus, a 'natural' estimation process is to choose that value of \(\theta\) that would maximize the probability that we would actually observe \(O\).</p>
           <p>Find the parameter values \(\theta\) that maximize the following function:</p>
           <p>$$L(\theta|O) = P(O|\theta)$$</p>
           <p><strong>L(\theta|O)</strong> is called the likelihood function. Notice that by definition, the likelihood function is conditioned on the observed \(O\) and that it is a function of the unknown parameters \(\theta\).</p>
            </li>
           <li><b>Continuous Random Variable</b>:In the continuous case, the situation is similar with one important difference. We can no longer talk about the probability that we observed \(O\) given \(\theta\) because in the continuous case \(P(O|\theta) = 0\).</p>
           <p>Denote the probability density function (pdf) associated with the outcomes \(O\) as: \(f(O|\theta)\). Thus, in the continuous case, we estimate \(\theta\) given observed outcomes \(O\) by maximizing the following function:</p>
           <p>$$L(\theta|O) = f(O|\theta)$$</p>
 
          </li>
        </ul>

        <hr>













      </div>
     

        <br><br><br><br><br><br><br><br><br><br>


      </div>

    </div>

  
</body></html>